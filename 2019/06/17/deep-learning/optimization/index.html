<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Optimization Optimization 损失函数 损失函数 Loss function / 代价函数 Cost function 均方损失 交叉熵损失（cross-entropy loss) Softmax 函数 损失函数可视化   梯度下降 全局梯度下降　Batch Gradient Descent 随机梯度下降SGD　Stochastic Gradient Descent 小批量梯">
<meta name="keywords" content="dl,hexo-asset-image">
<meta property="og:type" content="article">
<meta property="og:title" content="Dropout">
<meta property="og:url" content="http:/2019/06/17/deep-learning/optimization/index.html">
<meta property="og:site_name" content="SAILOR">
<meta property="og:description" content="Optimization Optimization 损失函数 损失函数 Loss function / 代价函数 Cost function 均方损失 交叉熵损失（cross-entropy loss) Softmax 函数 损失函数可视化   梯度下降 全局梯度下降　Batch Gradient Descent 随机梯度下降SGD　Stochastic Gradient Descent 小批量梯">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/03b3eccf18ee3760e219f9f95ec14305_hd.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/1561034699845.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/94dd0714f65ef94b3cbfff4780b1988d_hd.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/1561035128653.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/1561035151992.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/v2-bcdf4fdd7d2f7a8784a2cfe098f14a8c_r-1561050312112.jpg">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/d8b52b9b9ca31e2132c436c39af2943c_hd.jpg">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/1561186124221.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/1561186187105.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/1561196522519.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/nesterov_update_vector.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/412afb713ddcff0ba9165ab026563304_hd.png">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/equation.svg">
<meta property="og:image" content="http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif">
<meta property="og:image" content="http:/2019/06/17/deep-learning/optimization/20160909001936276">
<meta property="og:updated_time" content="2019-10-08T17:30:37.230Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dropout">
<meta name="twitter:description" content="Optimization Optimization 损失函数 损失函数 Loss function / 代价函数 Cost function 均方损失 交叉熵损失（cross-entropy loss) Softmax 函数 损失函数可视化   梯度下降 全局梯度下降　Batch Gradient Descent 随机梯度下降SGD　Stochastic Gradient Descent 小批量梯">
<meta name="twitter:image" content="http:/2019/06/17/deep-learning/optimization/03b3eccf18ee3760e219f9f95ec14305_hd.png">






  <link rel="canonical" href="http:/2019/06/17/deep-learning/optimization/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Dropout | SAILOR</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SAILOR</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">BLOG</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/06/17/deep-learning/optimization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Dropout

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-17 00:00:00" itemprop="dateCreated datePublished" datetime="2019-06-17T00:00:00+08:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-09 01:30:37" itemprop="dateModified" datetime="2019-10-09T01:30:37+08:00">2019-10-09</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             阅读次数： 
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><ul>
<li><a href="#Optimization">Optimization</a></li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a><ul>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-function--%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-Cost-function">损失函数 Loss function / 代价函数 Cost function</a></li>
<li><a href="#%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1">均方损失</a></li>
<li><a href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1cross-entropy-loss">交叉熵损失（cross-entropy loss)</a></li>
<li><a href="#Softmax-%E5%87%BD%E6%95%B0">Softmax 函数</a></li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8F%AF%E8%A7%86%E5%8C%96">损失函数可视化</a></li>
</ul>
</li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D">梯度下降</a><ul>
<li><a href="#%E5%85%A8%E5%B1%80%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Batch-Gradient-Descent">全局梯度下降　Batch Gradient Descent</a></li>
<li><a href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD-Stochastic-Gradient-Descent">随机梯度下降SGD　Stochastic Gradient Descent</a></li>
<li><a href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%EF%BC%ADini-batch-Gradient-Descent">小批量梯度下降　Ｍini-batch Gradient Descent</a></li>
<li><a href="#Challange">Challange</a></li>
</ul>
</li>
<li><a href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">优化算法</a><ul>
<li><a href="#%E7%89%9B%E9%A1%BF%E6%B3%95">牛顿法</a></li>
<li><a href="#BFGS-L-BFGS">BFGS, L-BFGS</a></li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E8%A1%B0%E5%87%8F%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB">梯度衰减(模拟退火)</a></li>
<li><a href="#%EF%BC%ADomentum">Ｍomentum</a></li>
<li><a href="#NAG-Neterov-Accelerated-Gradient">NAG Neterov Accelerated Gradient</a></li>
<li><a href="#Adagrad">Adagrad</a></li>
<li><a href="#Adadelta">Adadelta</a></li>
<li><a href="#RMSProp">RMSProp</a></li>
<li><a href="#Adam">Adam</a></li>
<li><a href="#Nadam">Nadam</a><ul>
<li><a href="#%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E5%8A%A8%E6%80%81%E5%9B%BE">各种优化方法动态图</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h4 id="损失函数-Loss-function-代价函数-Cost-function"><a href="#损失函数-Loss-function-代价函数-Cost-function" class="headerlink" title="损失函数 Loss function / 代价函数 Cost function"></a>损失函数 Loss function / 代价函数 Cost function</h4><p><img src="/2019/06/17/deep-learning/optimization/03b3eccf18ee3760e219f9f95ec14305_hd.png" alt="img"></p>
<h4 id="均方损失"><a href="#均方损失" class="headerlink" title="均方损失"></a>均方损失</h4><p>$$L(x)=\sum_{i}(y_i - y_j)^2$$</p>
<h4 id="交叉熵损失（cross-entropy-loss"><a href="#交叉熵损失（cross-entropy-loss" class="headerlink" title="交叉熵损失（cross-entropy loss)"></a>交叉熵损失（cross-entropy loss)</h4><p>$$L(x)=\sum_{i}p_i\log \frac{1}{q_i}$$</p>
<p>交叉熵作为代价函数可以避免均方代价函数带来的学习减速(nelson)</p>
<h4 id="Softmax-函数"><a href="#Softmax-函数" class="headerlink" title="Softmax 函数"></a>Softmax 函数</h4><p><img src="/2019/06/17/deep-learning/optimization/1561034699845.png" alt="1561034699845"></p>
<p>$$y_i=\frac{e^{z_i}}{\sum_{i}{e^{z_i}}}$$</p>
<p>Softmax 分类器为每一个分类都提供了“可能性（概率）”</p>
<p>输出值$z_i$ =&gt; 概率$y_i$</p>
<p><strong>决策函数</strong>：$$\hat{y}=argmax_{i}(y=y_i|Z)$$  (y所属类别$\hat{y}$)</p>
<h4 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h4><p>颜色越深，损失函数越小</p>
<p><img src="/2019/06/17/deep-learning/optimization/94dd0714f65ef94b3cbfff4780b1988d_hd.png" alt="img"></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>损失函数 L 的 3D 图：</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561035128653.png" alt="1561035128653"></p>
<p>Gradient: 损失函数 L 在某一点的梯度（斜率，一阶导数）</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561035151992.png" alt="1561035151992"></p>
<p><img src="/2019/06/17/deep-learning/optimization/v2-bcdf4fdd7d2f7a8784a2cfe098f14a8c_r-1561050312112.jpg" alt="preview"></p>
<p>梯度下降法：某一点沿着斜坡在当前点梯度最大的方向($f’(x)$, 下降最快的方向)移动一个步长(learning rate)，在下一次更新中就会更接近最小点。</p>
<p>$$w = w - \eta dw$$</p>
<p><img src="/2019/06/17/deep-learning/optimization/d8b52b9b9ca31e2132c436c39af2943c_hd.jpg" alt="img"></p>
<p>白箭头为（负梯度方向），是损失函数下降最陡峭的方向。沿着梯度方向逐步下降更新参数，就是深度学习中的梯度下降学习方法。</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561186124221.png" alt="1561186124221"></p>
<h4 id="全局梯度下降-Batch-Gradient-Descent"><a href="#全局梯度下降-Batch-Gradient-Descent" class="headerlink" title="全局梯度下降　Batch Gradient Descent"></a>全局梯度下降　Batch Gradient Descent</h4><p>$$\theta = \theta-\eta \nabla_{\theta} J(\theta)$$</p>
<p>优点：保证每次更新梯度都朝着正确的方向进行，保证收敛到局部最小点</p>
<p>缺点：每次需计算整个训练集数据，成本比较高</p>
<h4 id="随机梯度下降SGD-Stochastic-Gradient-Descent"><a href="#随机梯度下降SGD-Stochastic-Gradient-Descent" class="headerlink" title="随机梯度下降SGD　Stochastic Gradient Descent"></a>随机梯度下降SGD　Stochastic Gradient Descent</h4><p>$$\theta = \theta-\eta \nabla_{\theta} J(\theta;x_i,y_i)$$</p>
<p>优点：每次只选择一个样本来学习，成本低，可以进行在线更新; 不容易陷入某个局部</p>
<p>缺点：优化过程中波动较大，收敛速度慢；可能在沟壑两边持续震荡，停留在一个局部最优点</p>
<h4 id="小批量梯度下降-Mini-batch-Gradient-Descent"><a href="#小批量梯度下降-Mini-batch-Gradient-Descent" class="headerlink" title="小批量梯度下降　Ｍini-batch Gradient Descent"></a>小批量梯度下降　Ｍini-batch Gradient Descent</h4><p>$$\theta = \theta-\eta \nabla_{\theta} J(\theta;x_{i:i+n},y_{i,i+n})$$</p>
<p>n的大小通常为50-256</p>
<p>优点：相对于随机梯度下降(SGD)，降低收敛波动性（降低参数更新的方差），使得收敛过程更加稳定。相对于全量梯度下降，提高了每次学习的速度。</p>
<h4 id="Challange"><a href="#Challange" class="headerlink" title="Challange"></a>Challange</h4><ul>
<li><strong>学习速率</strong>的选择，过小则收敛很慢，过大则在极值点附近震荡</li>
<li>学习速率调整（模拟退火）一般使用事先设定的策略或者每次迭代中衰减一个阈值。都需要事先固定设置，无法<strong>自适应</strong>数据集的特点</li>
<li>所有的<strong>参数每次更新都使用相同的学习速率</strong>。如果数据特征稀疏或者取值空间分布不同，就不应该使用同样的学习速率，对于很少出现的特征应该使用一个较大的学习速率</li>
<li>对于非凸损失函数，容易陷入局部最小点，更严重的问题在于<strong>鞍点</strong>，附近点的梯度在所有维度上都接近于０</li>
</ul>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>$x = x -[Hf(x)]^{-1}\nabla f(x)$</p>
<p>$Hf(x)$: Hessian 矩阵，描述损失函数的局部曲率</p>
<p>$[Hf(x)]^{-1}$让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进</p>
<p>而且，没有学习率这个超参数</p>
<p>但是，Hessian矩阵计算成本很高</p>
<h4 id="BFGS-L-BFGS"><a href="#BFGS-L-BFGS" class="headerlink" title="BFGS, L-BFGS"></a>BFGS, L-BFGS</h4><h4 id="梯度衰减-模拟退火"><a href="#梯度衰减-模拟退火" class="headerlink" title="梯度衰减(模拟退火)"></a>梯度衰减(模拟退火)</h4><ul>
<li><p>随步数减半：没５个周期减半，每20个周期减少到之前0.1</p>
</li>
<li><p>指数衰减：$\alpha=\alpha_{0}e^{-kt}$</p>
</li>
<li>1/t衰减：$\alpha=\frac{\alpha_0}{1+kt}$</li>
</ul>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Ｍomentum"></a>Ｍomentum</h4><p>累积之前的下降方向，并略微偏向当前时刻的下降方向</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561186187105.png" alt="1561186187105"></p>
<p>引入动量，<strong>累积之前的动量(梯度的指数衰减)</strong>，速度越来越快。</p>
<p>$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)$$</p>
<p>$$\theta = \theta - v_t$$</p>
<p>$$\gamma$$通常使用0.9</p>
<p>加上动量后，就像从山顶往下滚的球。更新过程中，与上一次梯度方向相同的参数更新加强，在这个方向下降更快；与上一次梯度方向不同的参数更新减弱，在这个方向下降减慢。因此获得更快的收敛速度并减少震荡。</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561196522519.png" alt="1561196522519"></p>
<h4 id="NAG-Neterov-Accelerated-Gradient"><a href="#NAG-Neterov-Accelerated-Gradient" class="headerlink" title="NAG Neterov Accelerated Gradient"></a>NAG Neterov Accelerated Gradient</h4><p>往标准动量方法添加一个修正因子(当前梯度衰减$\eta v_{t-1}$)，阻止过快更新</p>
<p>$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta-\gamma v_{t-1})$$</p>
<p>$$\theta = \theta - v_t$$</p>
<p><img src="/2019/06/17/deep-learning/optimization/nesterov_update_vector.png" alt="SGD fluctuation"></p>
<p>核心思路：</p>
<p>当参数向量位于某个位置ｘ时，动量部分衰减$\gamma v_{t-1}$，向前一步$\theta-\gamma v_{t-1}$得到下一步要到达的位置，<strong>向前看</strong>在<strong>下一步</strong>的位置计算梯度。<img src="/2019/06/17/deep-learning/optimization/412afb713ddcff0ba9165ab026563304_hd.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure>
<p><strong>通过上面的两种方法，可以做到每次学习过程中能够根据损失函数的斜率做到自适应更新来加速SGD的收敛</strong>。</p>
<p>接下来引入二阶动量，希望同时解决<strong>不同参数应该使用不同学习速率的问题</strong>。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</p>
<p>怎么样去度量历史更新频率呢？</p>
<p>那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：</p>
<p>$$V_t=\sum_{\tau=1}^{t} g_{\tau}^2$$</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p><strong>累积平方梯度</strong></p>
<p>$$g_{t} = \nabla_{\theta} J(\theta)$$</p>
<p>$$r=r + g \odot g$$</p>
<p>$$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{r+\epsilon}}\odot g$$</p>
<p>优点：</p>
<ul>
<li><p>适合处理稀疏梯度。自适应不同的学习速率，对稀疏特征，得到更大的学习更新；对非稀疏特征，得到更小的学习更新　　特征出现越多，累积平方梯度ｒ越大，分母$\sqrt{r+\epsilon}$越大，更新越慢</p>
</li>
<li><p>前期$g_t$较小，校正因子较大，能够放大梯度，累积势越小，更新越快</p>
</li>
<li><p>后期$g_t$较大，校正因子较大，能够约束梯度，累积势越大，更新越慢</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>依赖学习速率</li>
<li>学习速率$\eta$不能过大，否则导致对梯度调节过大</li>
<li>单调的学习率被证明通常<strong>过于激进且过早停止学习</strong>。后期如果梯度的平方累积($r$)过大，导致更新很小，接近于０，使得训练提前结束</li>
</ul>
<p>由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。</p>
<p>引入指数移动平均值：</p>
<p>$$r_t=\gamma r_{t-1}+(1-\gamma)g_{t}^2$$</p>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>对Adagrad的扩展</p>
<p>$$g_{t} = \nabla_{\theta} J(\theta)$$</p>
<p>$$r=\gamma r + (1-\gamma) g \odot g$$</p>
<p>$$g’<em>t=\sqrt{\frac{\Delta x</em>{t-1} +\epsilon}{r+\epsilon}}\odot g$$                   # 使用<strong>状态变量$\Delta x_{t-1}$替代学习速率$\eta$</strong></p>
<p>$$\theta_{t+1}=\theta_{t}-g’_t$$</p>
<p>$$\Delta x_t=\gamma \Delta x_{t-1}+(1-\gamma)g’_t\odot g’<em>t $$   # 使用$\Delta x</em>{t-1}$记录自变量变化量$g′_t$按元素平方的指数加权移动平均</p>
<p>优点：</p>
<ul>
<li>不再依赖学习率，使用梯度平方的指数加权累积得到状态变量$\Delta x_{t-1}$替代学习速率$\eta$</li>
<li>训练初中期，加速效果好</li>
<li>训练后期，反复在局部最小值附近抖动</li>
</ul>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p><strong>累积指数加权的平方梯度</strong></p>
<p>$$E[g^2]<em>t=\gamma E[g^2]</em>{t-1}+(1-\gamma)g_t^2$$          # 梯度平方的滑动平均</p>
<p>$$\theta_{t-1}=\theta_{t} - \frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}\odot g_t$$</p>
<p>$\gamma=0.9, \eta=0.001$</p>
<ul>
<li>RMSProp 是 Adagrad 的一种发展，和 Adadelta 的变体，效果趋于二者之间，<strong>降低了 Adagrad 中学习速率衰减过快</strong>的问题</li>
<li>适合处理非平稳目标（RNN）</li>
</ul>
<p>接下来，整合前面所有方法，就得到 Adam。SGD-Momentum 在 SGD 的基础上增加了一阶动量，AdaGrad 和 AdaDelta 在 SGD 的基础上增加了二阶动量。结合一阶动量和二阶动量，就是 Adam (Adaptive+Momentum). 再加上 Neterov，就是 Nadam。</p>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adaptive Moment Estimation(Adam)</p>
<p><strong>累积指数加权的梯度和平方梯度</strong></p>
<p>$$m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t$$   # 一阶累积</p>
<p>$$r_t = \beta_2 r_{t-1} + (1-\beta_2)g_{t}^2$$　　# 二阶累积</p>
<p>$$\hat{m_t}=\frac{m_t}{1-\beta_1^t}$$　　　　　　　　　# 校正因子，$m_t$接近与０(初始0, $\beta$ 接近１)</p>
<p>$$\hat{v_t}=\frac{v_t}{1-\beta_2^t}$$</p>
<p>$$\theta_{t+1}=\theta_t - \eta\frac{\hat{m_t}}{\sqrt{\hat{v_t} + \epsilon}}$$</p>
<p>$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$</p>
<h4 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h4><p>$$g_t=\nabla_{\theta_t} J(\theta_t - m_t)$$</p>
<p>优点：</p>
<ul>
<li>结合 Adagrad 善于处理稀疏梯度和 RMSProp 善于处理非平稳目标的问题</li>
<li>适用于大数据集和高维空间</li>
</ul>
<p>缺点：</p>
<ul>
<li>二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 <img src="/2019/06/17/deep-learning/optimization/equation.svg" alt="[公式]"> 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。</li>
<li>可能错过全局最优解</li>
</ul>
<p><em>cs231n: The two recommended updates to use are either SGD+Nesterov Momentum or Adam</em></p>
<h3 id="各种优化方法动态图"><a href="#各种优化方法动态图" class="headerlink" title="各种优化方法动态图"></a>各种优化方法动态图</h3><p><img src="http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif" alt="SGD without momentum"></p>
<p> Adagrad、Adadelta与RMSprop在损失曲面上能够立即转移到正确的移动方向上达到快速的收敛。而Momentum 与NAG会导致偏离(off-track)。同时NAG能够在偏离之后快速修正其路线，因为其根据梯度修正来提高响应性</p>
<p><img src="/2019/06/17/deep-learning/optimization/20160909001936276" alt="saddle_point_evaluation_optimizers"></p>
<p>从上图可以看出，在鞍点（saddle points）处(即某些维度上梯度为零，某些维度上梯度不为零)，SGD、Momentum与NAG一直在鞍点梯度为零的方向上振荡，很难打破鞍点位置的对称性；Adagrad、RMSprop与Adadelta能够很快地向梯度不为零的方向上转移。<br>   从上面两幅图可以看出，自适应学习速率方法(Adagrad、Adadelta、RMSprop与Adam)在这些场景下具有更好的收敛速度与收敛性。</p>
<p>References:</p>
<ol>
<li><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
<li><a href="https://blog.csdn.net/heyongluoyao8/article/details/52478715" target="_blank" rel="noopener">梯度下降优化算法综述(1的翻译)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">一个框架看懂优化算法之异同 SGD/AdaGrad/Adam - Juliuszh的文章 - 知乎</a></li>
</ol>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/dl/" rel="tag"># dl</a>
          
            <a href="/tags/hexo-asset-image/" rel="tag"># hexo-asset-image</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/29/statitics/estimate/" rel="next" title="无偏估计和有偏估计">
                <i class="fa fa-chevron-left"></i> 无偏估计和有偏估计
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/06/17/deep-learning/rnn/" rel="prev" title="Dropout">
                Dropout <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Joseph.Fan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">4</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimization"><span class="nav-number">1.</span> <span class="nav-text">Optimization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#损失函数"><span class="nav-number">2.</span> <span class="nav-text">损失函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数-Loss-function-代价函数-Cost-function"><span class="nav-number">2.0.1.</span> <span class="nav-text">损失函数 Loss function / 代价函数 Cost function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#均方损失"><span class="nav-number">2.0.2.</span> <span class="nav-text">均方损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#交叉熵损失（cross-entropy-loss"><span class="nav-number">2.0.3.</span> <span class="nav-text">交叉熵损失（cross-entropy loss)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Softmax-函数"><span class="nav-number">2.0.4.</span> <span class="nav-text">Softmax 函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#损失函数可视化"><span class="nav-number">2.0.5.</span> <span class="nav-text">损失函数可视化</span></a></li></ol></li></ol><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降"><span class="nav-number">3.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#全局梯度下降-Batch-Gradient-Descent"><span class="nav-number">3.0.1.</span> <span class="nav-text">全局梯度下降　Batch Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机梯度下降SGD-Stochastic-Gradient-Descent"><span class="nav-number">3.0.2.</span> <span class="nav-text">随机梯度下降SGD　Stochastic Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#小批量梯度下降-Mini-batch-Gradient-Descent"><span class="nav-number">3.0.3.</span> <span class="nav-text">小批量梯度下降　Ｍini-batch Gradient Descent</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Challange"><span class="nav-number">3.0.4.</span> <span class="nav-text">Challange</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化算法"><span class="nav-number">4.</span> <span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#牛顿法"><span class="nav-number">4.0.1.</span> <span class="nav-text">牛顿法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BFGS-L-BFGS"><span class="nav-number">4.0.2.</span> <span class="nav-text">BFGS, L-BFGS</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度衰减-模拟退火"><span class="nav-number">4.0.3.</span> <span class="nav-text">梯度衰减(模拟退火)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Momentum"><span class="nav-number">4.0.4.</span> <span class="nav-text">Ｍomentum</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#NAG-Neterov-Accelerated-Gradient"><span class="nav-number">4.0.5.</span> <span class="nav-text">NAG Neterov Accelerated Gradient</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adagrad"><span class="nav-number">4.0.6.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adadelta"><span class="nav-number">4.0.7.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RMSProp"><span class="nav-number">4.0.8.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Adam"><span class="nav-number">4.0.9.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Nadam"><span class="nav-number">4.0.10.</span> <span class="nav-text">Nadam</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#各种优化方法动态图"><span class="nav-number">4.1.</span> <span class="nav-text">各种优化方法动态图</span></a></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joseph.Fan</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>




  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  


  


  





  

  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

</body>
</html>
