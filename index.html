<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="SAILOR">
<meta property="og:url" content="http:/index.html">
<meta property="og:site_name" content="SAILOR">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SAILOR">






  <link rel="canonical" href="http:/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>SAILOR</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SAILOR</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">BLOG</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/06/22/deep-learning/back_propagation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/22/deep-learning/back_propagation/" class="post-title-link" itemprop="url">Back Propagation</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-22 00:00:00" itemprop="dateCreated datePublished" datetime="2019-06-22T00:00:00+08:00">2019-06-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-23 22:46:21" itemprop="dateModified" datetime="2019-06-23T22:46:21+08:00">2019-06-23</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>Reference: <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html" target="_blank" rel="noopener">Understanding the backward pass through Batch Normalization Layer</a></p>
<h4 id="全连接网络数据指示图"><a href="#全连接网络数据指示图" class="headerlink" title="全连接网络数据指示图"></a>全连接网络数据指示图</h4><p><img src="/2019/06/22/deep-learning/back_propagation/1561185066736.png" alt="1561185066736"></p>
<p>$$w = w - \eta dw$$</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/06/19/deep-learning/batch_normalization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/19/deep-learning/batch_normalization/" class="post-title-link" itemprop="url">Batch Normalization</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-19 00:00:00" itemprop="dateCreated datePublished" datetime="2019-06-19T00:00:00+08:00">2019-06-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-23 17:55:54" itemprop="dateModified" datetime="2019-06-23T17:55:54+08:00">2019-06-23</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>批量归一化可以理解为在网络的每一层之前都做预处理，减少之前网络权重对数据的影响，保持每一层输出数据的分布（均值和标准差），使输出适应下一层网络，也使得每一层数据相对独立。</p>
<p><img src="/2019/06/19/deep-learning/batch_normalization/1560779540116.png" alt="1560779540116"></p>
<h3 id="Internal-Co-variate-Shift"><a href="#Internal-Co-variate-Shift" class="headerlink" title="Internal Co-variate Shift"></a>Internal Co-variate Shift</h3><p>Reference: <a href="https://zhuanlan.zhihu.com/p/34879333" target="_blank" rel="noopener">Batch Normalization原理与实战</a></p>
<p>随着训练的进行，网络中的参数也随着梯度下降在不停更新。一方面，当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大（类似蝴蝶效应）；另一方面，参数的变化导致每一层的输入分布会发生改变，进而下一层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。</p>
<p>原作定义：在深层网络训练的过程中，由于网络中参数变化而引起内部结点数据分布发生变化的这一过程被称作Internal Covariate Shift。</p>
<p>随着梯度下降的进行，每一层的参数$W^{[l]}$与$b^{[l]}$都会被更新，那么$Z^{[l]}$的分布也就发生了改变，进而$A^{[l]}$也同样出现分布的改变。而$A^{[l]}$作为第 $l+1$ 层的输入，意味着 $l+1$ 层需要去不停适应这种数据分布的变化，这一过程叫做 Interval Covariate Shift.</p>
<h4 id="带来的问题："><a href="#带来的问题：" class="headerlink" title="带来的问题："></a>带来的问题：</h4><ol>
<li>上层网络需要不停调整来适应输入数据分布的变化，导致网络学习速度的降低</li>
<li>网络的训练过程容易陷入梯度饱和区，减缓网络收敛速度（sigmoid, tanh）。 $Z^{[l]}$会逐渐更新并变大，陷入梯度饱和区。可以通过Normalization 使得激活函数输入分布在一个稳定的空间来避免他们陷入梯度饱和区。</li>
</ol>
<h4 id="如何减缓-Interval-Covariate-Shift"><a href="#如何减缓-Interval-Covariate-Shift" class="headerlink" title="如何减缓 Interval Covariate Shift"></a>如何减缓 Interval Covariate Shift</h4><ol>
<li><p>白化。成本高，改变了网络每一层分布导致数据表达的特征信息丢失</p>
<ul>
<li>使得输入特征分布具有相同的均值与方差。其中PCA白化保证了所有特征分布均值为0，方差为1</li>
<li>去除特征之间的相关性</li>
</ul>
</li>
<li><p>Batch Normalization   简化加改进版的白化</p>
<ul>
<li>简化。让每个特征都有均值为0，方差为1的分布就OK。</li>
<li>白化操作减弱了网络中每一层输入数据表达能力，那我就再加个线性变换操作，让这些数据再能够尽可能恢复本身的表达能力就好了。</li>
</ul>
<p><img src="/2019/06/19/deep-learning/batch_normalization/1560779531173.png" alt="1560779531173"></p>
</li>
</ol>
<p>BN 引入了两个可学习的参数 $\gamma$ 和 $\beta$（<strong>变换重构</strong>）。这两个参数的引入是为了恢复数据本身的表达能力，对规范后的数据进行线性变换，即<strong>$y_i = \gamma \hat{x_i} + \beta_i$</strong>。 特别的，当 $\gamma^2=\sigma ^2$（方差）, $\beta = \mu$ （均值）时，可以实现等价变换并且保留原始输入特征的分布信息。</p>
<h4 id="Batch-Normalization-的作用"><a href="#Batch-Normalization-的作用" class="headerlink" title="Batch Normalization 的作用"></a>Batch Normalization 的作用</h4><ol>
<li><p>使得网络中每层输入数据的分布相对稳定，加快模型学习速度</p>
</li>
<li><p>使得模型对参数不那么敏感，减小初始化参数对模型学习的影响，可以选择更大的初始化值，学习率选择范围更大</p>
<p>当学习率设置太高时，会使得参数更新步伐过大，容易出现震荡和不收敛。但是使用BN的网络将不会受到参数数值大小的影响。BN抑制了参数微小变化随着网络层数加深被放大的问题，使得网络对参数大小的适应能力更强</p>
</li>
<li><p>缓解梯度消失的问题</p>
</li>
<li><p>正则化效果，mini-batch 的mean/variance 作为总体样本的抽样估计，引入随机噪声</p>
</li>
</ol>
<p><strong>BN通过将每一层网络的输入进行normalization，保证输入分布的均值与方差固定在一定范围内，减少了网络中的Internal Covariate Shift问题，并在一定程度上缓解了梯度消失，加速了模型收敛；并且BN使得网络对参数、激活函数更加具有鲁棒性，降低了神经网络模型训练和调参的复杂度；最后BN训练过程中由于使用mini-batch的mean/variance作为总体样本统计量估计，引入了随机噪声，在一定程度上对模型起到了正则化的效果。</strong></p>
<h3 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        mu = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line">        x_norm = (x - mu) / np.sqrt(var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line"></span><br><span class="line">        cache = (x, mu, var, eps, x_norm, gamma, beta, out)</span><br><span class="line">        </span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * mu</span><br><span class="line">        running_var  = momentum * running_var  + (<span class="number">1</span> - momentum) * var</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        x_norm = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_norm + beta</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<h4 id="反向传播指示图"><a href="#反向传播指示图" class="headerlink" title="反向传播指示图"></a>反向传播指示图</h4><p><img src="/2019/06/19/deep-learning/batch_normalization/BNcircuit-1561283729951.png" alt="BNcircuit"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/06/17/deep-learning/dropout/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/17/deep-learning/dropout/" class="post-title-link" itemprop="url">Dropout</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-17 00:00:00" itemprop="dateCreated datePublished" datetime="2019-06-17T00:00:00+08:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-22 13:47:51" itemprop="dateModified" datetime="2019-06-22T13:47:51+08:00">2019-06-22</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><h2 id="Dropout-1"><a href="#Dropout-1" class="headerlink" title="Dropout"></a>Dropout</h2><p><img src="/2019/06/17/deep-learning/dropout/nn_cs231n_note/63fcf4cc655cb04f21a37e86aca333cf_hd.png" alt="img"></p>
<ol>
<li>Bagging 集成模型，随机抽样神经网络的子集。很多个共享参数的子网络组成。</li>
<li>增强单个神经元独立学习特征的能力，减少神经元之间的依赖（避免学习某些固定组合才产生的特征，有意识的让神经网络去学习一些普遍的共性）</li>
<li>加性噪声</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">反向随机失活: 推荐实现方式.</span></span><br><span class="line"><span class="string">在训练的时候drop和调整数值范围，测试时不做任何事.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">p = <span class="number">0.5</span> 	<span class="comment"># 激活神经元的概率. p值更高 = 随机失活更弱</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_step</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 3层neural network的前向传播</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)</span><br><span class="line">  <span class="comment"># 神经元以p的概率失活 [0, 1]随机分布 P(rand(x)) &lt; p = p</span></span><br><span class="line">  <span class="comment"># 第一个随机失活掩码. 注意/p! inverted dropout, 保持当前层输出期望一致</span></span><br><span class="line">  mask1 = (np.random.rand(*H1.shape) &lt; p) / p</span><br><span class="line">  H1 *= mask1                                  <span class="comment"># dropout!</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  mask2 = (np.random.rand(*H2.shape) &lt; p) / p  <span class="comment"># 第二个随机失活掩码. 注意/p!</span></span><br><span class="line">  H2 *= mask2 <span class="comment"># drop!</span></span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 反向传播:计算梯度... (略)</span></span><br><span class="line">  <span class="comment"># 进行参数更新... (略)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(X)</span>:</span></span><br><span class="line">  <span class="comment"># 前向传播时模型集成</span></span><br><span class="line">  H1 = np.maximum(<span class="number">0</span>, np.dot(W1, X) + b1)       <span class="comment"># 不用数值范围调整了</span></span><br><span class="line">  H2 = np.maximum(<span class="number">0</span>, np.dot(W2, H1) + b2)</span><br><span class="line">  out = np.dot(W3, H2) + b3</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/06/17/deep-learning/nn_cs231n_note/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/17/deep-learning/nn_cs231n_note/" class="post-title-link" itemprop="url">Neuron Network</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-17 00:00:00" itemprop="dateCreated datePublished" datetime="2019-06-17T00:00:00+08:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-23 15:37:24" itemprop="dateModified" datetime="2019-06-23T15:37:24+08:00">2019-06-23</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="深度学习和神经网络-CS231n-Note"><a href="#深度学习和神经网络-CS231n-Note" class="headerlink" title="深度学习和神经网络(CS231n Note)"></a>深度学习和神经网络(CS231n Note)</h2><ul>
<li><a href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%92%8C%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCS231n-Note">深度学习和神经网络(CS231n Note)</a></li>
<li><a href="#1-%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%95%B0%E5%AD%A6%E6%A8%A1%E5%9E%8BNeuron-Network-Unit">1. 神经元模型与数学模型（Neuron Network Unit）</a></li>
<li><a href="#2-%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0">2. 常用激活函数</a><ul>
<li><a href="#sigmoid">sigmoid</a></li>
<li><a href="#tanh">tanh</a></li>
<li><a href="#relu">relu</a></li>
<li><a href="#Leaky-Relu">Leaky Relu</a></li>
<li><a href="#ELU">ELU</a></li>
<li><a href="#Maxout">Maxout</a></li>
</ul>
</li>
<li><a href="#3-%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">3. 数据预处理</a><ul>
<li><a href="#%E5%BD%92%E4%B8%80%E5%8C%96-Normalization">归一化 Normalization</a></li>
<li><a href="#PCA-%E7%99%BD%E5%8C%96%E5%BE%88%E5%B0%91%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E4%BD%BF%E7%94%A8">PCA 白化（很少在深度学习中使用）</a></li>
<li><a href="#CIFAR-%E6%95%B0%E6%8D%AEPCA">CIFAR 数据PCA</a></li>
</ul>
</li>
<li><a href="#4-%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96">4. 权重初始化</a><ul>
<li><a href="#%E5%B0%8F%E9%9A%8F%E6%9C%BA%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96">小随机数初始化</a></li>
<li><a href="#%E4%BD%BF%E7%94%A8frac1sqrtn-%E6%A0%A1%E5%87%86%E6%96%B9%E5%B7%AE">使用$\frac{1}{\sqrt{n}} $校准方差</a></li>
<li><a href="#He-Normal">He Normal</a></li>
<li><a href="#%E5%81%8F%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96-biases">偏置初始化 biases</a></li>
</ul>
</li>
<li><a href="#5-%E6%AD%A3%E5%88%99%E5%8C%96">5. 正则化</a><ul>
<li><a href="#L1-%E6%AD%A3%E5%88%99">L1 正则</a></li>
<li><a href="#L2%E6%AD%A3%E5%88%99">L2正则</a></li>
</ul>
</li>
</ul>
<h2 id="1-神经元模型与数学模型（Neuron-Network-Unit）"><a href="#1-神经元模型与数学模型（Neuron-Network-Unit）" class="headerlink" title="1. 神经元模型与数学模型（Neuron Network Unit）"></a>1. 神经元模型与数学模型（Neuron Network Unit）</h2><p>大脑的基本计算单位是神经元（neuron<strong>）</strong>。人类的神经系统中大约有860亿个神经元，它们被大约$10^{14}-10^{15}$个<strong>突触（synapses）</strong>连接起来。下面图表的左边展示了一个生物学的神经元，右边展示了一个常用的数学模型。每个神经元都从它的<strong>树突</strong>获得输入信号，然后沿着它唯一的<strong>轴突（axon）</strong>产生输出信号。轴突在末端会逐渐分枝，通过突触和其他神经元的树突相连。</p>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/d0cbce2f2654b8e70fe201fec2982c7d_hd.png" alt="img"></p>
<p>神经元（Neuron）通过树突（Dendrites）接收输入信号，沿着轴突（axon）产生输出信号。轴突在末端分叉，通过突触和其他神经元的树突相连。</p>
<p>输入信号（$x_0,x_1,…,x_n$）传递到其他神经元的树突，基于突触的突触强度相乘（$w_0x_0,w_1x_1,…,w_nx_n$）。突触的强度（权重$w$）可以控制一个神经元对另一个神经元的影响强度，使其兴奋（正权重）或抑制（负权重）。输出信号如果高于阈值，则神经元激活（对应于激活函数$f(\sum_{i}w_ix_i+b)$）。</p>
<ol>
<li><p>1 多层感知机</p>
<p>输入层  -&gt; 隐藏层 -&gt; 输出层</p>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/ccb56c1fb267bc632d6d88459eb14ace_hd.png" alt="img"></p>
</li>
</ol>
<h2 id="2-常用激活函数"><a href="#2-常用激活函数" class="headerlink" title="2. 常用激活函数"></a>2. 常用激活函数</h2><table>
<thead>
<tr>
<th></th>
<th>函数</th>
<th>值域</th>
<th>导数</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>sigmoid</td>
<td>$ \sigma(x)=\frac{1}{1+e^{-x}} $</td>
<td>[0,1]</td>
<td>[0, 0.25]</td>
<td>分类概率</td>
</tr>
<tr>
<td>tanh</td>
<td>$tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$</td>
<td>[-1,1]</td>
<td>[0, 1]</td>
<td></td>
</tr>
<tr>
<td>relu</td>
<td>$relu(x)=max(0, x)$</td>
<td>$[0,+\infty]$</td>
<td>$f’_x=\begin{cases}0, x&lt;0\1, x&gt;0\undefinded, x=0\end{cases}$</td>
</tr>
</tbody>
</table>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>优点：</p>
<ol>
<li>分类概率</li>
</ol>
<p>缺点</p>
<ol>
<li>梯度容易饱和而丢失，激活函数在接近0,1时会饱和，如果权重过大，很容易失去梯度</li>
<li>函数不经过0，不以0点对称，在中间点0.5附近的的梯度较小（0.25），梯度总体较小，不利于梯度传播</li>
<li>激活值永远全为正（负），下一神经元的输入总是正数（负数），则反向传播过程中梯度更新呈z字型</li>
<li>exp指数函数计算复杂</li>
</ol>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/1560776135809.png" alt="1560776135809"></p>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>$tanh(x)=2\sigma(2x)-1$</p>
<p>优点：</p>
<ol>
<li>范围更大 [0, 1]</li>
<li>以 0 为中心点</li>
<li>中间部分梯度更大，有利于梯度传播</li>
</ol>
<p>缺点：</p>
<ol>
<li>梯度饱和而丢失的情况仍然存在</li>
</ol>
<h4 id="relu"><a href="#relu" class="headerlink" title="relu"></a>relu</h4><p>优点</p>
<ol>
<li>收敛速度更快（e.g. 6x than sigmoid/tanh)</li>
<li>计算简单</li>
<li>梯度不会饱和</li>
</ol>
<p>缺点</p>
<ol>
<li>可能导致部分神经元“死掉”，永远不会被激活。输出值始终为负，激活值为0，梯度为0，反向传播不更新此神经元的梯度。降低学习率来降低神经元“死掉”的概率。</li>
</ol>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/677187e96671a4cac9c95352743b3806_hd.png" alt="677187e96671a4cac9c95352743b3806_hd"></p>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/42.png" alt="img"></p>
<h4 id="Leaky-Relu"><a href="#Leaky-Relu" class="headerlink" title="Leaky Relu"></a>Leaky Relu</h4><p>解决Relu死亡的问题</p>
<p>$$f(x)=\begin{cases}x, x&gt;0\\alpha x, x&lt;0\end{cases}$$, $\alpha=0.01​$</p>
<h4 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h4><p>$$f(x)=\begin{cases} x, x&gt;0\ \alpha(e^x-1), x\leq0\end{cases}$$</p>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/1560776449074.png" alt="1560776449074"></p>
<h4 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h4><p>$$max(w_{1}^Tx+b_1, w_{2}^Tx+b_2)$$</p>
<h2 id="3-数据预处理"><a href="#3-数据预处理" class="headerlink" title="3. 数据预处理"></a>3. 数据预处理</h2><h3 id="归一化-Normalization"><a href="#归一化-Normalization" class="headerlink" title="归一化 Normalization"></a>归一化 Normalization</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 数据归一化</span><br><span class="line">X = X / np.std(X, axis=0)</span><br><span class="line"># 维度归一化</span><br><span class="line">X = X / np.std(X, axis=1)</span><br></pre></td></tr></table></figure>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/e743b6777775b1671c3b5503d7afbbc4_hd.png" alt="img"></p>
<h3 id="PCA-白化（很少在深度学习中使用）"><a href="#PCA-白化（很少在深度学习中使用）" class="headerlink" title="PCA 白化（很少在深度学习中使用）"></a>PCA 白化（很少在深度学习中使用）</h3><p>PCA/白化。<strong>左边</strong>是二维的原始数据。<strong>中间</strong>：经过PCA操作的数据。可以看出数据首先是零中心的，然后变换到了数据协方差矩阵的基准轴上。这样就对数据进行了解相关（协方差矩阵变成对角阵）。<strong>右边</strong>：每个维度都被特征值调整数值范围，将数据协方差矩阵变为单位矩阵。从几何上看，就是对数据在各个方向上拉伸压缩，使之变成服从高斯分布的一个数据点分布。</p>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/aae11de6e6a29f50d46b9ea106fbb02a_hd.png" alt="img"></p>
<h3 id="CIFAR-数据PCA"><a href="#CIFAR-数据PCA" class="headerlink" title="CIFAR 数据PCA"></a>CIFAR 数据PCA</h3><p>nx3072 维向量（图片32x32x3）,协方差矩阵：3072x3072</p>
<p><img src="/2019/06/17/deep-learning/nn_cs231n_note/8608c06086fc196228f4dda78499a2d9_hd.png" alt="img"></p>
<p>1: 49张图片。2: 3072个特征值向量中的前144个。3: 49张PCA降维的图片（U.transpose()[:144,:]）。4: 白化后的数据。144个维度的方差都压缩到相同的数值范围（U.transpose()[:144,:]）。现在较低的频率（代表了大多数方差）可以忽略不计了，较高的频率（代表相对少的方差）就被夸大了。</p>
<h2 id="4-权重初始化"><a href="#4-权重初始化" class="headerlink" title="4. 权重初始化"></a>4. 权重初始化</h2><h4 id="小随机数初始化"><a href="#小随机数初始化" class="headerlink" title="小随机数初始化"></a>小随机数初始化</h4><p>基于均值为0，标准差为1的高斯分布</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = 0.01 * np.random.randn(N, D)</span><br></pre></td></tr></table></figure>
<h4 id="使用-frac-1-sqrt-n-校准方差"><a href="#使用-frac-1-sqrt-n-校准方差" class="headerlink" title="使用$\frac{1}{\sqrt{n}} $校准方差"></a>使用$\frac{1}{\sqrt{n}} $校准方差</h4><p>数据量增大，随机初始化的神经元输出数据分布的方差也增大</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(N, D) / np.sqrt(N)</span><br></pre></td></tr></table></figure>
<h4 id="He-Normal"><a href="#He-Normal" class="headerlink" title="He Normal"></a>He Normal</h4><p>网络中神经元的方差应该是$\frac{2}{n}$</p>
<p>当前的推荐是使用ReLU激活函数，并且使用<strong>w = np.random.randn(n) * sqrt(2.0/n)</strong>来进行权重初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.randn(n) * sqrt(<span class="number">2.0</span>/n)</span><br></pre></td></tr></table></figure>
<h4 id="偏置初始化-biases"><a href="#偏置初始化-biases" class="headerlink" title="偏置初始化 biases"></a>偏置初始化 biases</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">b = np.zeros(n,)</span><br></pre></td></tr></table></figure>
<h2 id="5-正则化"><a href="#5-正则化" class="headerlink" title="5. 正则化"></a>5. 正则化</h2><h4 id="L1-正则"><a href="#L1-正则" class="headerlink" title="L1 正则"></a>L1 正则</h4><h4 id="L2正则"><a href="#L2正则" class="headerlink" title="L2正则"></a>L2正则</h4><p><img src="/2019/06/17/deep-learning/nn_cs231n_note/1561037588255.png" alt="1561037588255"></p>
<p>References:</p>
<ol>
<li><a href="https://zhuanlan.zhihu.com/p/21930884" target="_blank" rel="noopener">cs231n课程笔记中文翻译</a></li>
<li><a href="https://github.com/theBigDataDigest/Stanford-CS231n-assignments-in-Chinese" target="_blank" rel="noopener">cs231n Assignments 解答 - 大数据文摘</a><br>3.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/06/17/deep-learning/optimization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/17/deep-learning/optimization/" class="post-title-link" itemprop="url">Dropout</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-06-17 00:00:00" itemprop="dateCreated datePublished" datetime="2019-06-17T00:00:00+08:00">2019-06-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-23 15:35:33" itemprop="dateModified" datetime="2019-06-23T15:35:33+08:00">2019-06-23</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/dl/" itemprop="url" rel="index"><span itemprop="name">dl</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h2><ul>
<li><a href="#Optimization">Optimization</a></li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0">损失函数</a><ul>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-Loss-function--%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-Cost-function">损失函数 Loss function / 代价函数 Cost function</a></li>
<li><a href="#%E5%9D%87%E6%96%B9%E6%8D%9F%E5%A4%B1">均方损失</a></li>
<li><a href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1cross-entropy-loss">交叉熵损失（cross-entropy loss)</a></li>
<li><a href="#Softmax-%E5%87%BD%E6%95%B0">Softmax 函数</a></li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8F%AF%E8%A7%86%E5%8C%96">损失函数可视化</a></li>
</ul>
</li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D">梯度下降</a><ul>
<li><a href="#%E5%85%A8%E5%B1%80%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Batch-Gradient-Descent">全局梯度下降　Batch Gradient Descent</a></li>
<li><a href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8DSGD-Stochastic-Gradient-Descent">随机梯度下降SGD　Stochastic Gradient Descent</a></li>
<li><a href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%EF%BC%ADini-batch-Gradient-Descent">小批量梯度下降　Ｍini-batch Gradient Descent</a></li>
<li><a href="#Challange">Challange</a></li>
</ul>
</li>
<li><a href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95">优化算法</a><ul>
<li><a href="#%E7%89%9B%E9%A1%BF%E6%B3%95">牛顿法</a></li>
<li><a href="#BFGS-L-BFGS">BFGS, L-BFGS</a></li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E8%A1%B0%E5%87%8F%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB">梯度衰减(模拟退火)</a></li>
<li><a href="#%EF%BC%ADomentum">Ｍomentum</a></li>
<li><a href="#NAG-Neterov-Accelerated-Gradient">NAG Neterov Accelerated Gradient</a></li>
<li><a href="#Adagrad">Adagrad</a></li>
<li><a href="#Adadelta">Adadelta</a></li>
<li><a href="#RMSProp">RMSProp</a></li>
<li><a href="#Adam">Adam</a></li>
<li><a href="#Nadam">Nadam</a><ul>
<li><a href="#%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E5%8A%A8%E6%80%81%E5%9B%BE">各种优化方法动态图</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h4 id="损失函数-Loss-function-代价函数-Cost-function"><a href="#损失函数-Loss-function-代价函数-Cost-function" class="headerlink" title="损失函数 Loss function / 代价函数 Cost function"></a>损失函数 Loss function / 代价函数 Cost function</h4><p><img src="/2019/06/17/deep-learning/optimization/03b3eccf18ee3760e219f9f95ec14305_hd.png" alt="img"></p>
<h4 id="均方损失"><a href="#均方损失" class="headerlink" title="均方损失"></a>均方损失</h4><p>$$L(x)=\sum_{i}(y_i - y_j)^2$$</p>
<h4 id="交叉熵损失（cross-entropy-loss"><a href="#交叉熵损失（cross-entropy-loss" class="headerlink" title="交叉熵损失（cross-entropy loss)"></a>交叉熵损失（cross-entropy loss)</h4><p>$$L(x)=\sum_{i}p_i\log \frac{1}{q_i}$$</p>
<p>交叉熵作为代价函数可以避免均方代价函数带来的学习减速</p>
<h4 id="Softmax-函数"><a href="#Softmax-函数" class="headerlink" title="Softmax 函数"></a>Softmax 函数</h4><p><img src="/2019/06/17/deep-learning/optimization/1561034699845.png" alt="1561034699845"></p>
<p>$$y_i=\frac{e^{z_i}}{\sum_{i}{e^{z_i}}}$$</p>
<p>Softmax 分类器为每一个分类都提供了“可能性（概率）”</p>
<p>输出值$z_i$ =&gt; 概率$y_i$</p>
<h4 id="损失函数可视化"><a href="#损失函数可视化" class="headerlink" title="损失函数可视化"></a>损失函数可视化</h4><p>颜色越深，损失函数越小</p>
<p><img src="/2019/06/17/deep-learning/optimization/94dd0714f65ef94b3cbfff4780b1988d_hd.png" alt="img"></p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>损失函数 L 的 3D 图：</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561035128653.png" alt="1561035128653"></p>
<p>Gradient: 损失函数 L 在某一点的梯度（斜率，一阶导数）</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561035151992.png" alt="1561035151992"></p>
<p><img src="/2019/06/17/deep-learning/optimization/v2-bcdf4fdd7d2f7a8784a2cfe098f14a8c_r-1561050312112.jpg" alt="preview"></p>
<p>梯度下降法：某一点沿着斜坡在当前点梯度最大的方向($f’(x)​$)移动一个步长(learning rate)，在下一次更新中就会更接近最小点。</p>
<p>$$w = w - \eta dw$$</p>
<p><img src="/2019/06/17/deep-learning/optimization/d8b52b9b9ca31e2132c436c39af2943c_hd.jpg" alt="img"></p>
<p>白箭头为（负梯度方向），是损失函数下降最陡峭的方向。沿着梯度方向逐步下降更新参数，就是深度学习中的梯度下降学习方法。</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561186124221.png" alt="1561186124221"></p>
<h4 id="全局梯度下降-Batch-Gradient-Descent"><a href="#全局梯度下降-Batch-Gradient-Descent" class="headerlink" title="全局梯度下降　Batch Gradient Descent"></a>全局梯度下降　Batch Gradient Descent</h4><p>$$\theta = \theta-\eta \nabla_{\theta} J(\theta)$$</p>
<p>优点：保证每次更新梯度都朝着正确的方向进行，保证收敛到局部最小点</p>
<p>缺点：每次需计算整个训练集数据，成本比较高</p>
<h4 id="随机梯度下降SGD-Stochastic-Gradient-Descent"><a href="#随机梯度下降SGD-Stochastic-Gradient-Descent" class="headerlink" title="随机梯度下降SGD　Stochastic Gradient Descent"></a>随机梯度下降SGD　Stochastic Gradient Descent</h4><p>$$\theta = \theta-\eta \nabla_{\theta} J(\theta;x_i,y_i)$$</p>
<p>优点：每次只选择一个样本来学习，成本低，可以进行在线更新; 不容易陷入某个局部</p>
<p>缺点：优化过程中波动较大，收敛速度慢；可能在沟壑两边持续震荡，停留在一个局部最优点</p>
<h4 id="小批量梯度下降-Mini-batch-Gradient-Descent"><a href="#小批量梯度下降-Mini-batch-Gradient-Descent" class="headerlink" title="小批量梯度下降　Ｍini-batch Gradient Descent"></a>小批量梯度下降　Ｍini-batch Gradient Descent</h4><p>$$\theta = \theta-\eta \nabla_{\theta} J(\theta;x_{i:i+n},y_{i,i+n})$$</p>
<p>n的大小通常为50-256</p>
<p>优点：相对于随机梯度下降(SGD)，降低收敛波动性（降低参数更新的方差），使得收敛过程更加稳定。相对于全量梯度下降，提高了每次学习的速度。</p>
<h4 id="Challange"><a href="#Challange" class="headerlink" title="Challange"></a>Challange</h4><ul>
<li><strong>学习速率</strong>的选择，过小则收敛很慢，过大则在极值点附近震荡</li>
<li>学习速率调整（模拟退火）一般使用事先设定的策略或者每次迭代中衰减一个阈值。都需要事先固定设置，无法<strong>自适应</strong>数据集的特点</li>
<li>所有的<strong>参数每次更新都使用相同的学习速率</strong>。如果数据特征稀疏或者取值空间分布不同，就不应该使用同样的学习速率，对于很少出现的特征应该使用一个较大的学习速率</li>
<li>对于非凸损失函数，容易陷入局部最小点，更严重的问题在于<strong>鞍点</strong>，附近点的梯度在所有维度上都接近于０</li>
</ul>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><h4 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h4><p>$x = x -[Hf(x)]^{-1}\nabla f(x)$</p>
<p>$Hf(x)$: Hessian 矩阵，描述损失函数的局部曲率</p>
<p>$[Hf(x)]^{-1}$让最优化过程在曲率小的时候大步前进，在曲率大的时候小步前进</p>
<p>而且，没有学习率这个超参数</p>
<p>但是，Hessian矩阵计算成本很高</p>
<h4 id="BFGS-L-BFGS"><a href="#BFGS-L-BFGS" class="headerlink" title="BFGS, L-BFGS"></a>BFGS, L-BFGS</h4><h4 id="梯度衰减-模拟退火"><a href="#梯度衰减-模拟退火" class="headerlink" title="梯度衰减(模拟退火)"></a>梯度衰减(模拟退火)</h4><ul>
<li><p>随步数减半：没５个周期减半，每20个周期减少到之前0.1</p>
</li>
<li><p>指数衰减：$\alpha=\alpha_{0}e^{-kt}$</p>
</li>
<li>1/t衰减：$\alpha=\frac{\alpha_0}{1+kt}$</li>
</ul>
<h4 id="Momentum"><a href="#Momentum" class="headerlink" title="Ｍomentum"></a>Ｍomentum</h4><p>累积之前的下降方向，并略微偏向当前时刻的下降方向</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561186187105.png" alt="1561186187105"></p>
<p>引入动量，<strong>累积之前的动量(梯度的指数衰减)</strong>，速度越来越快。</p>
<p>$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)$$</p>
<p>$$\theta = \theta - v_t$$</p>
<p>$$\gamma$$通常使用0.9</p>
<p>加上动量后，就像从山顶往下滚的球。更新过程中，与上一次梯度方向相同的参数更新加强，在这个方向下降更快；与上一次梯度方向不同的参数更新减弱，在这个方向下降减慢。因此获得更快的收敛速度并减少震荡。</p>
<p><img src="/2019/06/17/deep-learning/optimization/1561196522519.png" alt="1561196522519"></p>
<h4 id="NAG-Neterov-Accelerated-Gradient"><a href="#NAG-Neterov-Accelerated-Gradient" class="headerlink" title="NAG Neterov Accelerated Gradient"></a>NAG Neterov Accelerated Gradient</h4><p>往标准动量方法添加一个修正因子(当前梯度衰减$\eta v_{t-1}$)，阻止过快更新</p>
<p>$$v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta-\gamma v_{t-1})$$</p>
<p>$$\theta = \theta - v_t$$</p>
<p><img src="/2019/06/17/deep-learning/optimization/nesterov_update_vector.png" alt="SGD fluctuation"></p>
<p>核心思路：</p>
<p>当参数向量位于某个位置ｘ时，动量部分衰减$\gamma v_{t-1}$，向前一步$\theta-\gamma v_{t-1}$得到下一步要到达的位置，<strong>向前看</strong>在<strong>下一步</strong>的位置计算梯度。<img src="/2019/06/17/deep-learning/optimization/412afb713ddcff0ba9165ab026563304_hd.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_ahead = x + mu * v</span><br><span class="line"><span class="comment"># 计算dx_ahead(在x_ahead处的梯度，而不是在x处的梯度)</span></span><br><span class="line">v = mu * v - learning_rate * dx_ahead</span><br><span class="line">x += v</span><br></pre></td></tr></table></figure>
<p><strong>通过上面的两种方法，可以做到每次学习过程中能够根据损失函数的斜率做到自适应更新来加速SGD的收敛</strong>。</p>
<p>接下来引入二阶动量，希望同时解决<strong>不同参数应该使用不同学习速率的问题</strong>。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。</p>
<p>怎么样去度量历史更新频率呢？</p>
<p>那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：</p>
<p>$$V_t=\sum_{\tau=1}^{t} g_{\tau}^2$$</p>
<h4 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h4><p><strong>累积平方梯度</strong></p>
<p>$$g_{t} = \nabla_{\theta} J(\theta)$$</p>
<p>$$r=r + g \odot g$$</p>
<p>$$\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{r+\epsilon}}\odot g$$</p>
<p>优点：</p>
<ul>
<li><p>适合处理稀疏梯度。自适应不同的学习速率，对稀疏特征，得到更大的学习更新；对非稀疏特征，得到更小的学习更新　　特征出现越多，累积平方梯度ｒ越大，分母$\sqrt{r+\epsilon}$越大，更新越慢</p>
</li>
<li><p>前期$g_t$较小，校正因子较大，能够放大梯度，累积势越小，更新越快</p>
</li>
<li><p>后期$g_t$较大，校正因子较大，能够约束梯度，累积势越大，更新越慢</p>
</li>
</ul>
<p>缺点：</p>
<ul>
<li>依赖学习速率</li>
<li>学习速率$\eta$不能过大，否则导致对梯度调节过大</li>
<li>单调的学习率被证明通常<strong>过于激进且过早停止学习</strong>。后期如果梯度的平方累积($r$)过大，导致更新很小，接近于０，使得训练提前结束</li>
</ul>
<p>由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。</p>
<p>引入指数移动平均值：</p>
<p>$$r_t=\gamma r_{t-1}+(1-\gamma)g_{t}^2$$</p>
<h4 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h4><p>对Adagrad的扩展</p>
<p>$$g_{t} = \nabla_{\theta} J(\theta)$$</p>
<p>$$r=\gamma r + (1-\gamma) g \odot g$$</p>
<p>$$g’<em>t=\sqrt{\frac{\Delta x</em>{t-1} +\epsilon}{r+\epsilon}}\odot g$$                   # 使用<strong>状态变量$\Delta x_{t-1}$替代学习速率$\eta$</strong></p>
<p>$$\theta_{t+1}=\theta_{t}-g’_t$$</p>
<p>$$\Delta x_t=\gamma \Delta x_{t-1}+(1-\gamma)g’_t\odot g’<em>t $$   # 使用$\Delta x</em>{t-1}$记录自变量变化量$g′_t$按元素平方的指数加权移动平均</p>
<p>优点：</p>
<ul>
<li>不再依赖学习率，使用梯度平方的指数加权累积得到状态变量$\Delta x_{t-1}$替代学习速率$\eta$</li>
<li>训练初中期，加速效果好</li>
<li>训练后期，反复在局部最小值附近抖动</li>
</ul>
<h4 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h4><p><strong>累积指数加权的平方梯度</strong></p>
<p>$$E[g^2]<em>t=\gamma E[g^2]</em>{t-1}+(1-\gamma)g_t^2$$          # 梯度平方的滑动平均</p>
<p>$$\theta_{t-1}=\theta_{t} - \frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}\odot g_t$$</p>
<p>$\gamma=0.9, \eta=0.001$</p>
<ul>
<li>RMSProp 是 Adagrad 的一种发展，和 Adadelta 的变体，效果趋于二者之间，<strong>降低了 Adagrad 中学习速率衰减过快</strong>的问题</li>
<li>适合处理非平稳目标（RNN）</li>
</ul>
<p>接下来，整合前面所有方法，就得到 Adam。SGD-Momentum 在 SGD 的基础上增加了一阶动量，AdaGrad 和 AdaDelta 在 SGD 的基础上增加了二阶动量。结合一阶动量和二阶动量，就是 Adam (Adaptive+Momentum). 再加上 Neterov，就是 Nadam。</p>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adaptive Moment Estimation(Adam)</p>
<p><strong>累积指数加权的梯度和平方梯度</strong></p>
<p>$$m_t=\beta_1 m_{t-1} + (1-\beta_1)g_t$$   # 一阶累积</p>
<p>$$r_t = \beta_2 r_{t-1} + (1-\beta_2)g_{t}^2$$　　# 二阶累积</p>
<p>$$\hat{m_t}=\frac{m_t}{1-\beta_1^t}$$　　　　　　　　　# 校正因子，$m_t$接近与０(初始0, $\beta$ 接近１)</p>
<p>$$\hat{v_t}=\frac{v_t}{1-\beta_2^t}$$</p>
<p>$$\theta_{t+1}=\theta_t - \eta\frac{\hat{m_t}}{\sqrt{\hat{v_t} + \epsilon}}$$</p>
<p>$\beta_1=0.9,\beta_2=0.999,\epsilon=10^{-8}$</p>
<h4 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h4><p>$$g_t=\nabla_{\theta_t} J(\theta_t - m_t)$$</p>
<p>优点：</p>
<ul>
<li>结合 Adagrad 善于处理稀疏梯度和 RMSProp 善于处理非平稳目标的问题</li>
<li>适用于大数据集和高维空间</li>
</ul>
<p>缺点：</p>
<ul>
<li>二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得 <img src="/2019/06/17/deep-learning/optimization/equation.svg" alt="[公式]"> 可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。</li>
<li>可能错过全局最优解</li>
</ul>
<p><em>cs231n: The two recommended updates to use are either SGD+Nesterov Momentum or Adam</em></p>
<h3 id="各种优化方法动态图"><a href="#各种优化方法动态图" class="headerlink" title="各种优化方法动态图"></a>各种优化方法动态图</h3><p><img src="http://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif" alt="SGD without momentum"></p>
<p> Adagrad、Adadelta与RMSprop在损失曲面上能够立即转移到正确的移动方向上达到快速的收敛。而Momentum 与NAG会导致偏离(off-track)。同时NAG能够在偏离之后快速修正其路线，因为其根据梯度修正来提高响应性</p>
<p><img src="/2019/06/17/deep-learning/optimization/20160909001936276" alt="saddle_point_evaluation_optimizers"></p>
<p>从上图可以看出，在鞍点（saddle points）处(即某些维度上梯度为零，某些维度上梯度不为零)，SGD、Momentum与NAG一直在鞍点梯度为零的方向上振荡，很难打破鞍点位置的对称性；Adagrad、RMSprop与Adadelta能够很快地向梯度不为零的方向上转移。<br>   从上面两幅图可以看出，自适应学习速率方法(Adagrad、Adadelta、RMSprop与Adam)在这些场景下具有更好的收敛速度与收敛性。</p>
<p>References:</p>
<ol>
<li><a href="http://ruder.io/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a></li>
<li><a href="https://blog.csdn.net/heyongluoyao8/article/details/52478715" target="_blank" rel="noopener">梯度下降优化算法综述(1的翻译)</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/32230623" target="_blank" rel="noopener">一个框架看懂优化算法之异同 SGD/AdaGrad/Adam - Juliuszh的文章 - 知乎</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/04/29/statitics/estimate/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/04/29/statitics/estimate/" class="post-title-link" itemprop="url">无偏估计和有偏估计</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-29 22:43:59" itemprop="dateCreated datePublished" datetime="2019-04-29T22:43:59+08:00">2019-04-29</time>
            

            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/03/25/statitics/entropy/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/25/statitics/entropy/" class="post-title-link" itemprop="url">entropy</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-25 23:18:46" itemprop="dateCreated datePublished" datetime="2019-03-25T23:18:46+08:00">2019-03-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-21 00:07:56" itemprop="dateModified" datetime="2019-06-21T00:07:56+08:00">2019-06-21</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">math</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h3><p>信息量是对事件发生概率的度量</p>
<p>$log\frac{1}{p}​$</p>
<p>一个事件发生的概率越低，则这个事件包含的信息量越大，比如说越稀奇新闻包含的信息量越大，因为这种新闻出现的概率低。</p>
<p>引用：一本五十万字的中文书平均有多少信息量？（$\log_2m$）  《数学之美 -  吴军》<br>我们知道常用的汉字（一级二级国标）大约有 7000 字。假如每个字等概率，那么我们大约需要 13 个比特（即 13 位二进制数）表示一个汉字［这里把汉字作为一个随机变量，那么汉字系统的熵就是约13bit］ 。但汉字的使用是不平衡的。实际上，前 10% 的汉字占文本的 95% 以上。因此，即使不考虑上下文的相关性，而只考虑每个汉字的独立的概率，那么，每个汉字的信息熵大约也只有 8-9 个比特。如果我们再考虑上下文相关性，每个汉字的信息熵［其实指的是汉字变量取特定汉字作为值时候具有的信息量］ 只有5比特左右。所以，一本五十万字的中文书，信息量大约是 250 万比特［这个时候的信息量就是每个汉字的信息量和数目相乘，指的都是汉字变量取具体值的信息量］ 。如果用一个好的算法压缩一下，整本书可以存成一个 320KB 的文件。如果我们直接用两字节的国标编码存储这本书，大约需要 1MB 大小，是压缩文件的三倍。这两个数量的差距，在信息论中称作“冗余度”（redundancy)。 需要指出的是我们这里讲的 250 万比特是个平均数，同样长度的书，所含的信息量可以差很多。如果一本书重复的内容很多，它的信息量就小，冗余度就大。</p>
<h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a>熵（Entropy）</h3><p>信息论中，熵是接收每条消息中包含的信息量($\log\frac{1}{p}​$)的平均值。</p>
<p>熵定义为信息的期望值（香农 shannon）：</p>
<p>$$\begin{align} H(X) &amp; = E[I(X)] \ &amp; = E[-ln(P(X))]  \ &amp; = -\sum\limits_{i}^n P(x_i)logP(x_i)\end{align} ​$$</p>
<p>表示样本的不确定性量度。在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。</p>
<p>单位：</p>
<p>熵的单位通常为比特, bit 或者sh(annon) (基于2)，但也用nat(基于自然对数)、Hart（基于10）计量，取决于定义用到对数的底。</p>
<ul>
<li>离散均匀分布</li>
</ul>
<p>$H(X)=log_2{m}$</p>
<p>例：抛掷三枚硬币</p>
<p>信息量：可以得到$2^3=8$种情况</p>
<p>不确定性：$log_2{8}=2\ bit$</p>
<ul>
<li>离散分布</li>
</ul>
<p>$H(X)=\sum\limits_{i}^nP(x_i)log\frac{1}{P(x_i)}$</p>
<p>例：选项ABCD</p>
<p>信息量：$H(X)=\sum\limits_{i}^n\frac{1}{4}log_2(\frac{1}{4})=2 bit$</p>
<p>给定A是错的，信息量变为：$H(X)=\sum\limits_{i}^n\frac{1}{3}log_2(\frac{1}{3})=1.585 bit$</p>
<p>“A是错的”，提供了$2-1.585=0.415bit​$的信息</p>
<p>例：编码</p>
<p>分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0。计算H(p)为1，即只需要1位编码即可识别A和B</p>
<p>分布q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(q)=2，即需要2位编码来识别A和B，还有C和D</p>
<h3 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a><a href="https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5" target="_blank" rel="noopener">交叉熵</a></h3><p>使用分布 q 来预测真实分布 p 的平均编码长度</p>
<p>$H(p,q)= E_p[\log  \frac{1}{q(i)}] = \sum\limits_{i}^{} p(i)*log\frac{1}{q(i)} $</p>
<p>交叉熵可以看作每个信息片段在错误分布 q 下的期望编码位长度，而信息实际分布为 p。</p>
<p>现有关于样本集的2个概率分布p和q，其中p为真实分布，q预测分布。</p>
<p>按照真实分布p来衡量识别一个样本的所需要的编码长度的期望(即平均编码长度)为：$H(p)=\sum\limits_{i}^{} p(i)*log\frac{1}{p(i)}$</p>
<p>使用预测分布q来表示来自真实分布p的平均编码长度，则应该是：$H(p,q)=\sum\limits_{i}^{} p(i)*log\frac{1}{q(i)} $</p>
<p>因为用q来编码的样本来自实际分布p，所以期望H(p,q)中概率是p(i)。H(p,q)我们称之为“交叉熵”。</p>
<p>比如含有4个字母(A,B,C,D)的数据集中，真实分布p=(1/2, 1/2, 0, 0)，即A和B出现的概率均为1/2，C和D出现的概率都为0，计算H(p)为1，即只需要1位编码即可识别A和B。如果使用分布Q=(1/4, 1/4, 1/4, 1/4)来编码则得到H(p,q)=2，即需要2位编码来识别A和B(C和D并不会出现)。</p>
<p>根据非真实分布q得到的平均编码长度H(p,q)大于根据真实分布p得到的平均编码长度H(p)。事实上，根据<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%2527_inequality" target="_blank" rel="noopener">Gibbs’ inequality</a>可知，H(p,q)&gt;=H(p)恒成立，当q为真实分布p时取等号。</p>
<h3 id="KL散度（相对熵）"><a href="#KL散度（相对熵）" class="headerlink" title="KL散度（相对熵）"></a>KL散度（相对熵）</h3><p>衡量分布 p 和 q 的差异，使用分布 q 来近似分布 p</p>
<p>$D_{KL}(p||q)=H(p,q)-H(p) = \sum\limits_{i}^{} p(i)<em>\log\frac{1}{q(i)} - \sum\limits_{i}^{} p(i) </em>\log\frac{1}{p(i)} = \sum\limits_{i}^{} p(i)*\log\frac{p(i)}{q(i)}$</p>
<p>我们将由q得到的平均编码长度比由p得到的平均编码长度多出的bit数称为“相对熵”，又被称为KL散度(Kullback–Leibler divergence，KLD) <a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Kullback%25E2%2580%2593Leibler_divergence" target="_blank" rel="noopener">Kullback–Leibler divergence</a>。它表示2个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小，二者分布相同则相对熵为0。</p>
<p>可以得到，交叉熵 $H(p,q)= E_p[\log  \frac{1}{q(i)}] = H(p) + D_{KL}(p||q)$</p>
<p>其中 $H(p)$ 是 $p$ 的<a href="https://zh.wikipedia.org/wiki/%E4%BF%A1%E6%81%AF%E7%86%B5" target="_blank" rel="noopener">熵</a>，$D_{KL}(p|q)$ 是从 $p$ 到 $q$ 的<a href="https://zh.wikipedia.org/w/index.php?title=KL%E6%95%A3%E5%BA%A6&amp;action=edit&amp;redlink=1" target="_blank" rel="noopener">KL散度</a>(也被称为<em>p</em>相对于<em>q</em>的<em>相对熵</em>)。</p>
<p>通常“相对熵”也可称为“交叉熵”，因为真实分布p是固定的，D(p||q)由H(p,q)决定。当然也有特殊情况，彼时2者须区别对待。</p>
<h3 id="交叉熵作为损失函数"><a href="#交叉熵作为损失函数" class="headerlink" title="交叉熵作为损失函数"></a>交叉熵作为损失函数</h3><p>交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。</p>
<p>交叉熵和均方误差都可以作为神经网络的损失函数，他们的区别在于：</p>
<ol>
<li>交叉熵适用于分类问题，结果是离散的类别（如图片分类），而均方误差适用于回归问题，结果是一个连续的数值（如雨量预测）【实际上均方误差也可以用于分类问题】</li>
<li>在使用 sigmod 激活函数时，如果使用均方误差作为损失函数，反向传播的导数（直接影响学习速度）会包含 sigmod函数的梯度，这个梯度随着变量的增大会趋向于0，导致学习速度迅速降低(梯度消失)；而如果使用交叉熵作为损失函数，就不存在这个问题，反向传播的导数包含 sigmod 函数，而不包含 sigmod 函数的导数。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># softmax_cross_entropy_with_logits计算后，矩阵的每一行数据计算出一个交叉熵，三行数据共计算出三个交叉熵</span></span><br><span class="line">cross_entropy_lst = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_)</span><br><span class="line"><span class="comment"># 通过reduce_sum进行累加，计算出一个batch的交叉熵</span></span><br><span class="line">cross_entropy = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class="line"><span class="comment"># 将batch里每条记录的交叉熵求均值，作为损失</span></span><br><span class="line">cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_))</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line">[ <span class="number">0.40760595</span>  <span class="number">0.40760595</span>  <span class="number">0.40760595</span>]</span><br><span class="line"><span class="number">1.22282</span></span><br><span class="line"><span class="number">0.407606</span></span><br></pre></td></tr></table></figure>
<p>References:</p>
<ul>
<li><a href="https://www.zhihu.com/question/41252833" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833</a></li>
<li><a href="https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BA%A4%E5%8F%89%E7%86%B5</a></li>
<li><a href="https://www.jianshu.com/p/92220ab37ea3" target="_blank" rel="noopener">https://www.jianshu.com/p/92220ab37ea3</a></li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/01/31/machine-learning/decision-tree/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/31/machine-learning/decision-tree/" class="post-title-link" itemprop="url">Decistion Tree</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-31 20:33:05" itemprop="dateCreated datePublished" datetime="2019-01-31T20:33:05+08:00">2019-01-31</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-21 00:07:57" itemprop="dateModified" datetime="2019-06-21T00:07:57+08:00">2019-06-21</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/ml/" itemprop="url" rel="index"><span itemprop="name">ml</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Part-4-决策树"><a href="#Part-4-决策树" class="headerlink" title="Part 4. 决策树"></a>Part 4. 决策树</h2><hr>
<p>Decision Tree　树形结构，根据损失函数最小化原则建立决策树模型</p>
<p><img src="/2019/01/31/machine-learning/decision-tree/tree_basic.png" alt="1548939696690"></p>
<p>if-then规则集合，每一条路径构建一条规则</p>
<ul>
<li><p>内部节点：特征/属性，规则的条件</p>
</li>
<li><p>叶节点：　类，规则的结论</p>
</li>
</ul>
<p>条件概率分布</p>
<p>将特征空间划分为互不相交的单元（cell）或区域（region）</p>
<p>$P(Y=+1|X=c)&gt;0.5$, 单元cell　属于正类</p>
<p><img src="/2019/01/31/machine-learning/decision-tree/feature-space.png" alt="1548940003249"></p>
<p><img src="/2019/01/31/machine-learning/decision-tree/1548940132559.png" alt="1548940132559"></p>
<p><img src="/2019/01/31/machine-learning/decision-tree/1548940093157.png" alt="1548940093157"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http:/2019/01/06/statitics/Statistics/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Joseph.Fan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SAILOR">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/06/statitics/Statistics/" class="post-title-link" itemprop="url">Statistic</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-06 23:18:46" itemprop="dateCreated datePublished" datetime="2019-01-06T23:18:46+08:00">2019-01-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-29 22:43:59" itemprop="dateModified" datetime="2019-04-29T22:43:59+08:00">2019-04-29</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Beyes"><a href="#Beyes" class="headerlink" title="Beyes"></a>Beyes</h3><p>$$P(H|D) = \frac{P(H) \cdot P(D|H)}{P(D)}​$$</p>
<p>H: hyposis 假设事件，D: data 数据</p>
<ul>
<li>$P(H|D)$ : 后验概率, the probability of observing event H given that D is true</li>
<li>$P(H)​$ : 先验概率, the probability of observing event H</li>
<li>$P(D|H)$: 似然度　</li>
<li>$P(D)$ :   the probability of data D</li>
</ul>
<p>例：在判断垃圾邮件的算法中:<br>  $P(H)​$ : 所有邮件中，垃圾邮件的概率。<br>  $P(D)​$ : 出现某个单词的概率。<br>  $P(D|H)​$ : 垃圾邮件中，出现某个单词的概率。<br>  $P(H|D)​$ : 出现某个单词的邮件，是垃圾邮件的概率。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Joseph.Fan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">分类</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">标签</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joseph.Fan</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.7.0</div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="post-meta-divider">|</span>
  

  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>




  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

</body>
</html>
